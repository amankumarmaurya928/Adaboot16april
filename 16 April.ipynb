{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1404aa4-2621-4d97-9cf5-7d6bb79b674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine \\n   learning software, called machine learning models, on labeled data to make guesses about unlabeled data.\\n   '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Boosting is a method used in machine learning to reduce errors in predictive data analysis. Data scientists train machine \n",
    "   learning software, called machine learning models, on labeled data to make guesses about unlabeled data.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d5b043f-b782-4901-93b3-3321623718c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The advantages of boosting techniques\\n   The key benefits of boosting include:\\n   Ease of Implementation: Boosting can be used with several hyper-parameter tuning options to improve fitting.\\n   Reduction of bias: Boosting algorithms combine multiple weak learners in a sequential method, iteratively improving upon\\n   observations.\\nThe limitations of boosting algorithm\\n   Complex: It is complex to handle all models' working and increase the data's weight from every error. Algorithms are\\n   complicated to run in real time.\\n   Dependency: Each successor model is dependent on the last model which may result in an error.\\n   \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The advantages of boosting techniques\n",
    "   The key benefits of boosting include:\n",
    "   Ease of Implementation: Boosting can be used with several hyper-parameter tuning options to improve fitting.\n",
    "   Reduction of bias: Boosting algorithms combine multiple weak learners in a sequential method, iteratively improving upon\n",
    "   observations.\n",
    "The limitations of boosting algorithm\n",
    "   Complex: It is complex to handle all models' working and increase the data's weight from every error. Algorithms are\n",
    "   complicated to run in real time.\n",
    "   Dependency: Each successor model is dependent on the last model which may result in an error.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a9b809-3b3a-478a-9e36-4f2219c3d1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training\\n   errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each\\n   model tries to compensate for the weaknesses of its predecessor.\\n   '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training\n",
    "   errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each\n",
    "   model tries to compensate for the weaknesses of its predecessor.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce96847-f93a-4e0f-ba06-f044ba31a55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three types of Algorithms available :\\n1. Adaptive boosting Algorithm,\\n2. Gradient,\\n3. XG Boosting algorithm.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''There are three types of Algorithms available :\n",
    "1. Adaptive boosting Algorithm,\n",
    "2. Gradient,\n",
    "3. XG Boosting algorithm.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5ebebb-5edd-4065-8310-697f42e493df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 4 boosting algorithms in machine learning\\n1. GBM, \\n2. XGBoost,\\n3. LightGBM \\n4. CatBoost.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''The 4 boosting algorithms in machine learning\n",
    "1. GBM, \n",
    "2. XGBoost,\n",
    "3. LightGBM \n",
    "4. CatBoost.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48aad42-2328-4adc-aed4-e3a7eafe4836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training \\n   errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each \\n   model tries to compensate for the weaknesses of its predecessor.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize training \n",
    "   errors. In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is, each \n",
    "   model tries to compensate for the weaknesses of its predecessor.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558cbb19-fec3-4b6c-8754-2e11a16d8721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning. It\\n   is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights assigned to incorrectly \\n   classified instances.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning. It\n",
    "   is called Adaptive Boosting as the weights are re-assigned to each instance, with higher weights assigned to incorrectly \n",
    "   classified instances.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd475cad-41ab-4b52-b4ad-22de54443a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The error function that AdaBoost uses is an exponential loss function.\\n   In case of Adaptive Boosting or AdaBoost, it minimises the exponential loss function that can make the algorithm sensitive \\n   to the outliers. With Gradient Boosting, any differentiable loss function can be utilised. Gradient Boosting algorithm is\\n   more robust to outliers than AdaBoost.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "'''The error function that AdaBoost uses is an exponential loss function.\n",
    "   In case of Adaptive Boosting or AdaBoost, it minimises the exponential loss function that can make the algorithm sensitive \n",
    "   to the outliers. With Gradient Boosting, any differentiable loss function can be utilised. Gradient Boosting algorithm is\n",
    "   more robust to outliers than AdaBoost.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8593096-1b0b-438c-aab9-dc4540fbe8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is done by making misclassified cases to be updated with increased weights after an iteration. Increased weights would\\n   make our learning algorithm pay higher attention to these observations in the next iteration.\\n   Initially all the data points have equal probability of getting selected, that is each data point has a weight equal to 1/N.\\n   In each iteration the weight of a data point gets changed in such a way, that it gets decreased, if it is correctly \\n   classified by the model generated in that iteration and increased otherwise.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q9\n",
    "'''This is done by making misclassified cases to be updated with increased weights after an iteration. Increased weights would\n",
    "   make our learning algorithm pay higher attention to these observations in the next iteration.\n",
    "   Initially all the data points have equal probability of getting selected, that is each data point has a weight equal to 1/N.\n",
    "   In each iteration the weight of a data point gets changed in such a way, that it gets decreased, if it is correctly \n",
    "   classified by the model generated in that iteration and increased otherwise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e3c1497-ebf8-46cd-bbb4-919bfd7cb1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Few important parameters of AdaBoost are :\\n      Base_estimator: It is a weak learner used to train the model. n_estimators: Number of weak learners to train in each \\n      iteration. learning_rate: It contributes to the weights of weak learners.\\n  An important hyperparameter for Adaboost is n_estimator. Often by changing the number of base models or weak learners we can\\n  adjust the accuracy of the model. The number of trees added to the model must be high for the model to work well, often \\n  hundreds, if not thousands.24-Aug-2020\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q10\n",
    "'''Few important parameters of AdaBoost are :\n",
    "      Base_estimator: It is a weak learner used to train the model. n_estimators: Number of weak learners to train in each \n",
    "      iteration. learning_rate: It contributes to the weights of weak learners.\n",
    "  An important hyperparameter for Adaboost is n_estimator. Often by changing the number of base models or weak learners we can\n",
    "  adjust the accuracy of the model. The number of trees added to the model must be high for the model to work well, often \n",
    "  hundreds, if not thousands.24-Aug-2020\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cce9b-7224-4305-94c9-2c381eb7a15a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
